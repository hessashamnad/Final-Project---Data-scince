{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from seaborn import barplot,countplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, udf, isnan, count, when, desc, sort_array, asc, avg, lag, floor\n",
    "from pyspark.sql.types import IntegerType, DateType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler,Normalizer,StandardScaler\n",
    "from pyspark.ml.feature import MinMaxScaler #used because won't distort binary vars\n",
    "from pyspark.sql.types import DoubleType,FloatType\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import datetime\n",
    "from pyspark.ml import Pipeline \n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, LinearSVC, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.functions import min as Fmin, max as Fmax, sum as Fsum, round as Fround\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Identify Features\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.json('mini_sparkify_event_data.json')\n",
    "\n",
    "# Print schema for future reference\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty userIds: 0\n"
     ]
    }
   ],
   "source": [
    "#clean dataset with empty userid\n",
    "df = df.filter(df.userId != \"\")\n",
    "print(f\"Empty userIds: {df.filter(df.userId == '').count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# observation\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30'),\n",
       " Row(artist='Five Iron Frenzy', auth='Logged In', firstName='Micah', gender='M', itemInSession=79, lastName='Long', length=236.09424, level='free', location='Boston-Cambridge-Newton, MA-NH', method='PUT', page='NextSong', registration=1538331630000, sessionId=8, song='Canada', status=200, ts=1538352180000, userAgent='\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.103 Safari/537.36\"', userId='9'),\n",
       " Row(artist='Adam Lambert', auth='Logged In', firstName='Colin', gender='M', itemInSession=51, lastName='Freeman', length=282.8273, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Time For Miracles', status=200, ts=1538352394000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30'),\n",
       " Row(artist='Enigma', auth='Logged In', firstName='Micah', gender='M', itemInSession=80, lastName='Long', length=262.71302, level='free', location='Boston-Cambridge-Newton, MA-NH', method='PUT', page='NextSong', registration=1538331630000, sessionId=8, song='Knocking On Forbidden Doors', status=200, ts=1538352416000, userAgent='\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.103 Safari/537.36\"', userId='9'),\n",
       " Row(artist='Daft Punk', auth='Logged In', firstName='Colin', gender='M', itemInSession=52, lastName='Freeman', length=223.60771, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Harder Better Faster Stronger', status=200, ts=1538352676000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at first 5 rows for sample\n",
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Amount of users \n",
    "df.select(\"userId\").dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2312"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Amount of sessions\n",
    "df.select(\"sessionId\").dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = udf(lambda x: int(x==\"Cancellation Confirmation\"), IntegerType())\n",
    "downgrade_churn = udf(lambda x: int(x==\"Submit Downgrade\"), IntegerType())\n",
    "\n",
    "df = df.withColumn(\"downgraded\", downgrade_churn(\"page\")).withColumn(\"cancelled\", churn(\"page\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+\n",
      "|summary|            userId|        downgraded|         cancelled|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|  count|               225|               225|               225|\n",
      "|   mean|65391.013333333336|              0.28|0.2311111111111111|\n",
      "| stddev|105396.47791907164|0.5876709477736184|0.4224832108996327|\n",
      "|    min|                10|                 0|                 0|\n",
      "|    max|                99|                 3|                 1|\n",
      "+-------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Distribution of user downgrades and cancellations\n",
    "df.select(['userId', 'downgraded', 'cancelled'])\\\n",
    "    .groupBy('userId').sum()\\\n",
    "    .withColumnRenamed('sum(downgraded)', 'downgraded')\\\n",
    "    .withColumnRenamed('sum(cancelled)', 'cancelled').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowvalue = Window.partitionBy(\"userId\").orderBy(desc(\"ts\")).rangeBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "\n",
    "df = df.withColumn(\"churn_phase\", Fsum(\"cancelled\").over(windowvalue))\\\n",
    "    .withColumn(\"downgrade_phase\", Fsum(\"downgraded\").over(windowvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_day = udf(lambda x: datetime.datetime.fromtimestamp(x/1000), DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "song=udf(lambda x : int(x=='NextSong'), IntegerType())\n",
    "home_visit=udf(lambda x : int(x=='Home'), IntegerType())\n",
    "df = df.withColumn('date', get_day(col('ts')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|   avg(songPlayed)|\n",
      "+------------------+\n",
      "|0.8112072039942939|\n",
      "+------------------+\n",
      "\n",
      "+------------------+\n",
      "|   avg(songPlayed)|\n",
      "+------------------+\n",
      "|0.8217840456084702|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col('churn_phase')==1).withColumn('songPlayed', song(col('page'))).agg({'songPlayed':'mean'}).show()\n",
    "df.filter(col('churn_phase')==0).withColumn('songPlayed', song(col('page'))).agg({'songPlayed':'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = lambda i: i * 86400 \n",
    "daywindow = Window.partitionBy('userId', 'date').orderBy(desc('ts')).rangeBetween(Window.unboundedPreceding, 0)\n",
    "get_day = udf(lambda x: datetime.datetime.fromtimestamp(x/1000), DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+\n",
      "|summary|           userId|            count|\n",
      "+-------+-----------------+-----------------+\n",
      "|  count|              499|              499|\n",
      "|   mean|77394.81563126252|72.93386773547094|\n",
      "| stddev|90869.89716037885|71.24764235703725|\n",
      "|    min|           100001|                1|\n",
      "|    max|               87|              346|\n",
      "+-------+-----------------+-----------------+\n",
      "\n",
      "+-------+------------------+-----------------+\n",
      "|summary|            userId|            count|\n",
      "+-------+------------------+-----------------+\n",
      "|  count|              2559|             2559|\n",
      "|   mean| 64501.19812426729|74.91754591637358|\n",
      "| stddev|114144.74005493976|74.02407951472078|\n",
      "|    min|                10|                1|\n",
      "|    max|                99|              360|\n",
      "+-------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#number of songs played daily\n",
    "df.filter((df.page=='NextSong')&(col('churn_phase')==1)).select('userId', 'page', 'ts')\\\n",
    "    .withColumn('date', get_day(col('ts'))).groupBy('userId', 'date').count().describe().show()\n",
    "\n",
    "df.filter((df.page=='NextSong')&(col('churn_phase')==0)).select('userId', 'page', 'ts')\\\n",
    "    .withColumn('date', get_day(col('ts'))).groupBy('userId', 'date').count().describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|       count(page)|\n",
      "+-------+------------------+\n",
      "|  count|                30|\n",
      "|   mean|1.0666666666666667|\n",
      "| stddev|0.2537081317024624|\n",
      "|    min|                 1|\n",
      "|    max|                 2|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------+-------------------+\n",
      "|summary|        count(page)|\n",
      "+-------+-------------------+\n",
      "|  count|                200|\n",
      "|   mean|                1.1|\n",
      "| stddev|0.31702131247412063|\n",
      "|    min|                  1|\n",
      "|    max|                  3|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#number of songs couldn't be played due to errors\n",
    "df.filter((df.page=='Error')&(df.churn_phase==1)).select('userId', 'page', 'ts', 'length')\\\n",
    "    .withColumn('date', get_day(col('ts')))\\\n",
    "    .groupBy('userId', 'date').agg({'page':'count'}).select('count(page)').describe().show()\n",
    "\n",
    "df.filter((df.page=='Error')&(df.churn_phase==0)).select('userId', 'page', 'ts', 'length')\\\n",
    "    .withColumn('date', get_day(col('ts')))\\\n",
    "    .groupBy('userId', 'date').agg({'page':'count'}).select('count(page)').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 21\n"
     ]
    }
   ],
   "source": [
    "#who cancelled subscriptions both free and paid\n",
    "print(df.filter((df.page=='Cancellation Confirmation') & (df.level=='paid')).count(),\n",
    "df.filter((df.page=='Cancellation Confirmation') & (df.level=='free')).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 52\n"
     ]
    }
   ],
   "source": [
    "#number of users who downgraded\n",
    "#number of users to cancel\n",
    "print(df.filter(col('downgraded')==1).select('userId').dropDuplicates().count(), \n",
    "      df.filter(col('cancelled')==1).select('userId').dropDuplicates().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#users who downgraded and cancelled\n",
    "df.select(['userId', 'downgraded', 'cancelled'])\\\n",
    "    .groupBy('userId').sum()\\\n",
    "    .withColumnRenamed('sum(downgraded)', 'downgraded')\\\n",
    "    .withColumnRenamed('sum(cancelled)', 'cancelled')\\\n",
    "    .filter((col(\"downgraded\")==1)&(col(\"cancelled\")==1))\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+\n",
      "|summary|           userId|       count(page)|\n",
      "+-------+-----------------+------------------+\n",
      "|  count|              171|               171|\n",
      "|   mean|62613.21052631579|1.3976608187134503|\n",
      "| stddev| 88778.9002607509|0.8078987426633197|\n",
      "|    min|           100001|                 1|\n",
      "|    max|               87|                 6|\n",
      "+-------+-----------------+------------------+\n",
      "\n",
      "+-------+------------------+------------------+\n",
      "|summary|            userId|       count(page)|\n",
      "+-------+------------------+------------------+\n",
      "|  count|               816|               816|\n",
      "|   mean| 63176.38602941176|1.4889705882352942|\n",
      "| stddev|115512.27837413888|0.8636495080491383|\n",
      "|    min|                10|                 1|\n",
      "|    max|                99|                 8|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#number of times user opted for help\n",
    "df.filter((df.page=='Help')&(df.churn_phase==1)).select('userId', 'page', 'ts', 'length')\\\n",
    "    .withColumn('date', get_day(col('ts')))\\\n",
    "    .groupBy('userId', 'date').agg({'page':'count'}).describe().show()\n",
    "\n",
    "df.filter((df.page=='Help')&(df.churn_phase==0)).select('userId', 'page', 'ts', 'length')\\\n",
    "    .withColumn('date', get_day(col('ts')))\\\n",
    "    .groupBy('userId', 'date').agg({'page':'count'}).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of users to cancel w/o downgrade\n",
    "df.select(['userId', 'downgraded', 'cancelled'])\\\n",
    "    .groupBy('userId').sum()\\\n",
    "    .withColumnRenamed('sum(downgraded)', 'downgraded')\\\n",
    "    .withColumnRenamed('sum(cancelled)', 'cancelled')\\\n",
    "    .filter((col(\"downgraded\")==0)&(col(\"cancelled\")==1))\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "#number of paid users to drop without downgrading -> big chunk of those\n",
    "print(df.filter((col('cancelled')==1) & (col('downgraded')==0) & (col('level')=='paid'))\\\n",
    "      .select('userId').dropDuplicates().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|avg(length)|\n",
      "+-----------+\n",
      "|       null|\n",
      "+-----------+\n",
      "\n",
      "+-----------------+\n",
      "|      avg(length)|\n",
      "+-----------------+\n",
      "|249.1171819778458|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#find those who done have diff listening habits?\n",
    "df.filter(col('cancelled')==1).agg({'length':'mean'}).show()\n",
    "df.filter(col('cancelled')==0).agg({'length':'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%matplot` not found.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb0d588c2b0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting a bar plot to show difference between Paid/Free customers churn\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "df_level_pd = df.filter('page == \"Cancellation Confirmation\"').groupby('level').count().toPandas()\n",
    "plt.title('Paid/Free customers churn')\n",
    "sns.barplot(data = df_level_pd, x = 'level', y = 'count', color = sns.color_palette()[1])\n",
    "\n",
    "ax.set(xticklabels=['Free Subscriptions', 'Paid Subscriptions'])\n",
    "\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xlabel('User Level')\n",
    "ax.legend()\n",
    "\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing userId and sessionId\n",
    "df = df.dropna(how = 'any', subset = ['userId','sessionId'])\n",
    "df = df[df.userId != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add downgrade flag\n",
    "df = df.withColumn('downgrade', when(df.page == 'Submit Downgrade', 1).otherwise(0))\n",
    "df = df.withColumn('user_downgrade', Fmax('downgrade').over(Window.partitionBy('UserId')))\n",
    "\n",
    "# add churn\n",
    "df = df.withColumn('churn', when(df.page == 'Cancellation Confirmation', 1).otherwise(0))\n",
    "df = df.withColumn('user_churn', Fmax('churn').over(Window.partitionBy('UserId')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%matplot` not found.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb0d57e6550>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "churn_gender_df = df.drop_duplicates(['userId', 'gender']).groupby(['user_churn', 'gender']).count().toPandas()\n",
    "churn_gender_df\n",
    "\n",
    "\n",
    "\n",
    "# Plotting a bar plot to show gender distribution between regular and churn users\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plt.title('Regular and churn Users by Gender')\n",
    "ax = sns.barplot(x = 'user_churn', y = 'count', data = churn_gender_df, hue = 'gender')\n",
    "\n",
    "ax.set(xticklabels=['Not Churned', 'Churned'])\n",
    "\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xlabel('User Churn')\n",
    "ax.legend()\n",
    "\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Marking the time and phase of churn for each user\n",
    "# This will help us find metrics before and after churn for users\n",
    "\n",
    "\n",
    "window = Window().partitionBy(\"userId\").orderBy(\"ts\").rangeBetween(Window.unboundedPreceding, 0)\n",
    "df = df.withColumn(\"churn_phase\", Fsum(\"cancelled\").over(window)).withColumn(\"downgrade_phase\", Fsum(\"downgraded\").over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|max(downgrade_phase)|\n",
      "+--------------------+\n",
      "|                   3|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg( Fmax(col(\"downgrade_phase\")) ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|level|        avg(count)|\n",
      "+-----+------------------+\n",
      "| free|215.33846153846153|\n",
      "| paid|1134.8597560975609|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.page == \"NextSong\").select([\"userId\", \"level\", \"ts\"]).groupby( [ \"level\", \"userId\" ] ).count().groupby(\"level\").mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|level|avgSongsPerSession|\n",
      "+-----+------------------+\n",
      "| free| 31.28982093144803|\n",
      "| paid|102.62525766710827|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of songs per session between paid and free\n",
    "\n",
    "#Lets first compute average number of songs per session for each user\n",
    "\n",
    "user_avg_songs_per_session = df.where(df.page == \"NextSong\") \\\n",
    "    .select([\"userId\", \"level\", \"sessionId\"]) \\\n",
    "    .groupby( [ \"level\", \"userId\", \"sessionId\" ] ).count() \\\n",
    "    .withColumnRenamed(\"count\", \"numSongsPerSession\") \\\n",
    "    .groupby( [ \"level\", \"userId\" ] ).agg( { \"numSongsPerSession\" : \"avg\"}) \\\n",
    "    .withColumnRenamed(\"avg(numSongsPerSession)\", \"avgSongsPerSession\")\n",
    "\n",
    "# Now lets average across all users for free and paid respectively\n",
    "user_avg_songs_per_session.groupby(\"level\").mean() \\\n",
    "    .withColumnRenamed(\"avg(avgSongsPerSession)\", \"avgSongsPerSession\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|level|    avgSongsPerDay|\n",
      "+-----+------------------+\n",
      "| free|32.318292019819815|\n",
      "| paid| 87.08996179115817|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of songs per day across free and paid\n",
    "\n",
    "# We first convert the timestamp to date ( in UTC ) and add it to the dataframe\n",
    "\n",
    "ts_date = udf(lambda x: datetime.datetime.utcfromtimestamp(x/1000), DateType() )\n",
    "df = df.withColumn(\"date\", ts_date(col(\"ts\")))\n",
    "\n",
    "#Compute number of songs per day per user\n",
    "\n",
    "user_avg_songs_per_day = df.where(df.page == \"NextSong\") \\\n",
    "    .select([\"userId\", \"level\", \"date\"]) \\\n",
    "    .groupby( [ \"level\", \"userId\", \"date\" ] ).count() \\\n",
    "    .withColumnRenamed(\"count\", \"numSongsPerDay\") \\\n",
    "    .groupby( [ \"level\", \"userId\" ] ).agg( { \"numSongsPerDay\" : \"avg\"}) \\\n",
    "    .withColumnRenamed(\"avg(numSongsPerDay)\", \"avgSongsPerDay\")\n",
    "\n",
    "# Now lets average across all users for free and paid respectively\n",
    "user_avg_songs_per_day.groupby(\"level\").mean() \\\n",
    "    .withColumnRenamed(\"avg(avgSongsPerDay)\", \"avgSongsPerDay\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+---------------------------+\n",
      "|max(pagenotfound_instances)|min(pagenotfound_instances)|\n",
      "+---------------------------+---------------------------+\n",
      "|                          7|                          1|\n",
      "+---------------------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of 404s per user\n",
    "\n",
    "user_pagenotfound = df.filter(df.status == 404).select(\"userId\", \"status\", \"ts\").groupby(\"userId\").count().withColumnRenamed(\"count\", \"pagenotfound_instances\")\n",
    "\n",
    "user_pagenotfound.agg( Fmax(col(\"pagenotfound_instances\")), Fmin(col(\"pagenotfound_instances\"))  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|max(error_instances)|min(error_instances)|\n",
      "+--------------------+--------------------+\n",
      "|                   7|                   1|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of errors per user\n",
    "\n",
    "user_error_pages = df.filter(df.page == \"Error\").select(\"userId\", \"status\", \"ts\").groupby(\"userId\").count().withColumnRenamed(\"count\", \"error_instances\")\n",
    "\n",
    "user_error_pages.agg( Fmax(col(\"error_instances\")), Fmin(col(\"error_instances\"))  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thumbs up and thumbs down per user\n",
    "\n",
    "user_thumbsup_perday = df.filter(df.page == \"Thumbs Up\") \\\n",
    "    .select(\"userId\", \"date\") \\\n",
    "    .groupby( [ \"userId\", \"date\" ]) \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"thumbsup\") \\\n",
    "    .groupby(\"userId\").mean() \\\n",
    "    .withColumnRenamed(\"avg(thumbsup)\", \"avg_thumbsup_per_day\")\n",
    "    \n",
    "user_thumbsdown_perday = df.filter(df.page == \"Thumbs Down\") \\\n",
    "    .select(\"userId\", \"date\") \\\n",
    "    .groupby( [ \"userId\", \"date\" ]) \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"thumbsdown\") \\\n",
    "    .groupby(\"userId\").mean() \\\n",
    "    .withColumnRenamed(\"avg(thumbsdown)\", \"avg_thumbsdown_per_day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+\n",
      "|max(help_visits)|min(help_visits)|\n",
      "+----------------+----------------+\n",
      "|              46|               1|\n",
      "+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of help page visits\n",
    "\n",
    "user_help_vists = df.filter(df.page == \"Help\").select(\"userId\", \"status\", \"ts\").groupby(\"userId\").count().withColumnRenamed(\"count\", \"help_visits\")\n",
    "\n",
    "user_help_vists.agg( Fmax(col(\"help_visits\")), Fmin(col(\"help_visits\"))  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of friend referrals\n",
    "\n",
    "user_num_friends = df.select([\"userId\", \"page\"]) \\\n",
    "    .filter(df.page == \"Add Friend\") \\\n",
    "    .groupby(\"userId\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"numFriendAdditions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstract the data fetching from disk and basic cleanup into a function\n",
    "\n",
    "def obtain_data(filepath):\n",
    "    '''\n",
    "    Given a file path:\n",
    "    \n",
    "    1. Read the events json file\n",
    "    2. Remove records with null or empty string userId's and sessionId's\n",
    "    '''\n",
    "    df = spark.read.json(filepath=filepath)\n",
    "    df = df.where( (df.userId.isNotNull()) & (df.sessionId.isNotNull()) & (df.userId != \"\") & (df.sessionId != \"\") )\n",
    "    return df    \n",
    "\n",
    "# Combine all the feature engineering and put in a single function\n",
    "\n",
    "def feature_engineering(events_df):\n",
    "    '''\n",
    "    Given an events spark data frame, compute the necessary features for\n",
    "    predicting churn using machine learning algorithms\n",
    "    \n",
    "    Inputs:\n",
    "        events_df: Spark data frame with the structure        \n",
    "         |-- artist: string (nullable = true)\n",
    "         |-- auth: string (nullable = true)\n",
    "         |-- firstName: string (nullable = true)\n",
    "         |-- gender: string (nullable = true)\n",
    "         |-- itemInSession: long (nullable = true)\n",
    "         |-- lastName: string (nullable = true)\n",
    "         |-- length: double (nullable = true)\n",
    "         |-- level: string (nullable = true)\n",
    "         |-- location: string (nullable = true)\n",
    "         |-- method: string (nullable = true)\n",
    "         |-- page: string (nullable = true)\n",
    "         |-- registration: long (nullable = true)\n",
    "         |-- sessionId: long (nullable = true)\n",
    "         |-- song: string (nullable = true)\n",
    "         |-- status: long (nullable = true)\n",
    "         |-- ts: long (nullable = true)\n",
    "         |-- userAgent: string (nullable = true)\n",
    "         |-- userId: string (nullable = true)            \n",
    "    \n",
    "    Output:\n",
    "        df_feature: Spark data frame with the following structure\n",
    "        root\n",
    "         |-- userId: string (nullable = true)\n",
    "         |-- downgraded: integer (nullable = true)\n",
    "         |-- cancelled: integer (nullable = true)\n",
    "         |-- avgSongsPerSession: double (nullable = true)\n",
    "         |-- avgSongsPerDay: double (nullable = true)\n",
    "         |-- avg_thumbsup_per_day: double (nullable = true)\n",
    "         |-- avg_thumbsup_per_day: double (nullable = true)\n",
    "         |-- error_instances: long (nullable = false)\n",
    "         |-- help_visits: long (nullable = false)\n",
    "         |-- numFriendAdditions: long (nullable = false)\n",
    "    '''\n",
    "    \n",
    "    churn = udf(lambda x: int(x == 'Cancellation Confirmation'), IntegerType())\n",
    "    downgrade_churn = udf(lambda x: int(x == 'Submit Downgrade'), IntegerType())    \n",
    "    signum = udf(lambda x: int(x>0), IntegerType())\n",
    "    ts_date = udf(lambda x: datetime.datetime.utcfromtimestamp(x/1000), DateType() )\n",
    "    proportion = udf(lambda x,y : x/y, FloatType())\n",
    "    \n",
    "    ## Base transformations to input df\n",
    "    events_df = events_df.withColumn(\"date\", ts_date(col(\"ts\")))       \n",
    "       \n",
    "    ## User level features    \n",
    "    \n",
    "    # Status of each user ( target variable )\n",
    "    user_status = events_df.select([\"userId\", \"downgraded\", \"cancelled\"]) \\\n",
    "        .groupby(\"userId\").sum() \\\n",
    "        .withColumnRenamed(\"sum(downgraded)\" , \"sum_downgraded\") \\\n",
    "        .withColumnRenamed(\"sum(cancelled)\" , \"sum_cancelled\") \\\n",
    "\n",
    "    user_status = user_status \\\n",
    "        .withColumn('downgraded', signum(col(\"sum_downgraded\"))) \\\n",
    "        .withColumn('cancelled', signum(col(\"sum_cancelled\"))) \\\n",
    "        .drop('sum_downgraded').drop('sum_cancelled')\n",
    "    # The signum function is used since there are users who have multiple downgrade events.\n",
    "        \n",
    "    #Number of songs per session per user\n",
    "    user_avg_songs_per_session = events_df.where(events_df.page == \"NextSong\") \\\n",
    "        .select([\"userId\", \"sessionId\"]) \\\n",
    "        .groupby( [ \"userId\", \"sessionId\" ] ).count() \\\n",
    "        .withColumnRenamed(\"count\", \"numSongsPerSession\") \\\n",
    "        .groupby( [ \"userId\" ] ).agg( { \"numSongsPerSession\" : \"avg\"}) \\\n",
    "        .withColumnRenamed(\"avg(numSongsPerSession)\", \"avgSongsPerSession\")\n",
    "    \n",
    "    #Number of songs per day per user\n",
    "    user_avg_songs_per_day = events_df.where(events_df.page == \"NextSong\") \\\n",
    "        .select([\"userId\", \"date\"]) \\\n",
    "        .groupby( [ \"userId\", \"date\" ] ).count() \\\n",
    "        .withColumnRenamed(\"count\", \"numSongsPerDay\") \\\n",
    "        .groupby( [ \"userId\" ] ).agg( { \"numSongsPerDay\" : \"avg\"}) \\\n",
    "        .withColumnRenamed(\"avg(numSongsPerDay)\", \"avgSongsPerDay\")\n",
    "    \n",
    "    #Number of thumbs ups per day per user\n",
    "    user_thumbsup_perday = events_df.filter(events_df.page == \"Thumbs Up\") \\\n",
    "        .select(\"userId\", \"date\") \\\n",
    "        .groupby( [ \"userId\", \"date\" ]) \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed(\"count\", \"thumbsup\") \\\n",
    "        .groupby(\"userId\").mean() \\\n",
    "        .withColumnRenamed(\"avg(thumbsup)\", \"avg_thumbsup_per_day\")\n",
    "    \n",
    "    #Number of thumbs down per day per user\n",
    "    user_thumbsdown_perday = events_df.filter(events_df.page == \"Thumbs Down\") \\\n",
    "        .select(\"userId\", \"date\") \\\n",
    "        .groupby( [ \"userId\", \"date\" ]) \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed(\"count\", \"thumbsdown\") \\\n",
    "        .groupby(\"userId\").mean() \\\n",
    "        .withColumnRenamed(\"avg(thumbsdown)\", \"avg_thumbsdown_per_day\")    \n",
    "\n",
    "    #Number of error pages encountered by user\n",
    "    user_error_pages = events_df.filter(events_df.page == \"Error\") \\\n",
    "        .select(\"userId\", \"status\", \"ts\") \\\n",
    "        .groupby(\"userId\") \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed(\"count\", \"error_instances\")\n",
    "    \n",
    "    # Number of help page visits\n",
    "    user_help_vists = events_df.filter(events_df.page == \"Help\") \\\n",
    "        .select(\"userId\", \"status\", \"ts\") \\\n",
    "        .groupby(\"userId\") \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed(\"count\", \"help_visits\")\n",
    "    \n",
    "    # Number of friend additions per user\n",
    "    user_num_friends = events_df.select([\"userId\", \"page\"]) \\\n",
    "        .filter(df.page == \"Add Friend\") \\\n",
    "        .groupby(\"userId\") \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed(\"count\", \"numFriendAdditions\")\n",
    "    \n",
    "    df_features = user_status \\\n",
    "        .join(user_avg_songs_per_session, on = \"userId\" , how = 'left') \\\n",
    "        .join(user_avg_songs_per_day, on = \"userId\", how = 'left') \\\n",
    "        .join(user_thumbsup_perday, on = \"userId\", how = 'left') \\\n",
    "        .join(user_thumbsdown_perday, on = \"userId\", how = 'left') \\\n",
    "        .join(user_error_pages, on = \"userId\", how = 'left') \\\n",
    "        .join(user_help_vists, on = \"userId\", how = 'left') \\\n",
    "        .join(user_num_friends, on = \"userId\", how = 'left')\n",
    "    \n",
    "    # Renaming the target column to label and casting it to DoubleType\n",
    "    df_features = df_features.withColumn( \"label\", df_features.cancelled.cast(DoubleType())).drop(\"cancelled\")\n",
    "    \n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------------------+------------------+--------------------+----------------------+---------------+-----------+------------------+-----+\n",
      "|userId|downgraded|avgSongsPerSession|    avgSongsPerDay|avg_thumbsup_per_day|avg_thumbsdown_per_day|error_instances|help_visits|numFriendAdditions|label|\n",
      "+------+----------+------------------+------------------+--------------------+----------------------+---------------+-----------+------------------+-----+\n",
      "|100010|         0|39.285714285714285|39.285714285714285|  2.8333333333333335|                  1.25|           null|          2|                 4|  0.0|\n",
      "|200002|         0|              64.5|55.285714285714285|                 3.0|                   3.0|           null|          2|                 4|  0.0|\n",
      "|   125|         0|               8.0|               8.0|                null|                  null|           null|       null|              null|  1.0|\n",
      "|   124|         0|145.67857142857142|         127.46875|   5.896551724137931|    1.8636363636363635|              6|         23|                74|  0.0|\n",
      "|    51|         0|             211.1| 162.3846153846154|   8.333333333333334|                   2.1|              1|         12|                28|  1.0|\n",
      "|     7|         0|21.428571428571427|             18.75|                1.75|                   1.0|              1|          1|                 1|  0.0|\n",
      "|    15|         0|136.71428571428572|100.73684210526316|                 5.4|                   1.4|              2|          8|                31|  0.0|\n",
      "|    54|         1| 81.17142857142858|              94.7|   6.037037037037037|    1.5263157894736843|              1|         17|                33|  1.0|\n",
      "|   155|         0|136.66666666666666|             102.5|   8.285714285714286|                   1.0|              3|          9|                11|  0.0|\n",
      "|100014|         0|42.833333333333336|42.833333333333336|                 3.4|                   1.0|           null|          2|                 6|  1.0|\n",
      "+------+----------+------------------+------------------+--------------------+----------------------+---------------+-----------+------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features = feature_engineering(df)\n",
    "df_features.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_scaling(df_features):\n",
    "    feature_cols = df_features.drop('userId', 'label').columns\n",
    "    #Replace nulls with zero for columns in feature_cols\n",
    "    df_features = df_features.fillna(0, subset = feature_cols )\n",
    "    \n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"FeatureVector\")\n",
    "    df_features = assembler.transform(df_features)\n",
    "    \n",
    "    scaler = StandardScaler(withMean = True, inputCol = \"FeatureVector\",  outputCol = \"ScaledFeatureVector\")\n",
    "    scalerModel = scaler.fit(df_features)\n",
    "    df_features = scalerModel.transform(df_features)\n",
    "    df_features_scaled = df_features.select([\"userId\", \"label\", \"ScaledFeatureVector\"])    \n",
    "    \n",
    "    return df_features_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_scaled = feature_scaling(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: string, label: double, ScaledFeatureVector: vector]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features_scaled.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|userId|label|ScaledFeatureVector                                                                                                                                              |\n",
      "+------+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|100010|0.0  |[-0.5264710031632485,-0.7392637500378801,-0.783710508690406,-0.7321501459351433,-0.36390803366118396,-0.7605559300887401,-0.6161090451316099,-0.7292340618073143]|\n",
      "|200002|0.0  |[-0.5264710031632485,-0.14759257929335814,-0.2848591481704853,-0.6465812558359041,1.8888671790512386,-0.7605559300887401,-0.6161090451316099,-0.7292340618073143]|\n",
      "|125   |1.0  |[-0.5264710031632485,-1.473405316004171,-1.7591430797070366,-2.1868212776222102,-1.9730331855986287,-0.7605559300887401,-0.892253537471475,-0.9235813095498617]  |\n",
      "|124   |0.0  |[-0.5264710031632485,1.7573204109365261,1.965678700112606,0.8405470410612189,0.4260261318353797,3.3138508382437974,2.2834081244369724,2.6718427736872665]        |\n",
      "|51    |1.0  |[-0.5264710031632485,3.292480822508503,3.0542928845404678,2.0916232273397517,0.7302970696562785,-0.08148813536665052,0.764613416567715,0.4368494246479705]       |\n",
      "|7     |0.0  |[-0.5264710031632485,-1.158294324219553,-1.423977321857715,-1.2883479315801984,-0.685733064048673,-0.08148813536665052,-0.7541812913015424,-0.8749944976142249]  |\n",
      "|15    |0.0  |[-0.5264710031632485,1.5469670626973269,1.1322256612011883,0.5856107615931411,-0.17081301542869073,0.597579659355439,0.21232443188798503,0.5826098604548812]     |\n",
      "|54    |1.0  |[1.8909978889128924,0.24361436476265175,0.9440077301102838,0.9126740748613443,-0.008206684285538196,-0.08148813536665052,1.4549746474173773,0.6797834843261549]  |\n",
      "|155   |0.0  |[-0.5264710031632485,1.545849647832842,1.1871977683637451,2.0671749730256836,-0.685733064048673,1.2766474540775286,0.3503966780579175,-0.38912637825785623]      |\n",
      "|100014|1.0  |[-0.5264710031632485,-0.6560163426337877,-0.6731020969084592,-0.44121591959773,-0.685733064048673,-0.7605559300887401,-0.6161090451316099,-0.6320604379360406]   |\n",
      "+------+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features_scaled.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation = df_features_scaled.randomSplit([0.70, 0.30], seed = 42)\n",
    "# test, validation = rest.randomSplit([0.50, 0.50], seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_threshold_by_f1_score(preds, labelCol = 'label', probabilityCol = 'probability', steps = 1000):\n",
    "    '''\n",
    "        Given prediction probabilities by a classiification model, find the best threshold that maximizes fscore\n",
    "        \n",
    "        Input:\n",
    "            preds - Spark dataframe with the following schema            \n",
    "             |-- userId: string (nullable = true)\n",
    "             |-- label: double (nullable = true)\n",
    "             |-- ScaledFeatureVector: vector (nullable = true)\n",
    "             |-- rawPrediction: vector (nullable = true)\n",
    "             |-- probability: vector (nullable = true)\n",
    "             |-- prediction: double (nullable = false)\n",
    "        \n",
    "        Output:\n",
    "            best_threshold - Best threshold in [0,1] that maximizes fscore\n",
    "            best_fscore - Best fscore achieved using the best_threshold\n",
    "    '''\n",
    "    labels = preds[labelCol]\n",
    "    probability = preds[probabilityCol]\n",
    "    fscores = []\n",
    "    thresholds = np.linspace(0,1,steps)\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        pred_labels = probability.apply(lambda x: np.float64(x > thresh))\n",
    "        fscores.append( f1_score(labels, pred_labels) )\n",
    "    \n",
    "    return ( thresholds[ np.argmax(fscores) ], np.max(fscores) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train, validation):\n",
    "    '''\n",
    "    Input:\n",
    "        ML model after fitting\n",
    "        \n",
    "        Given a model we find the best threshold that gives the best f-score on the valdation data set\n",
    "        Then we take this best threshold and apply that on the model predictions of the test data set.\n",
    "        On this set set of predictions on the test set we compute precision, recall, f-score etc.\n",
    "        \n",
    "        We also compute area under the ROC curve for train, validation and test datasets\n",
    "    \n",
    "    Output:\n",
    "        Metrics:\n",
    "            AUC for train, validation and test            \n",
    "            Best threshold based on validation\n",
    "            FScore using best threshold on validation, test            \n",
    "    '''\n",
    "    \n",
    "    model_preds_train = model.transform(train)\n",
    "    model_preds_valid = model.transform(validation)\n",
    "    \n",
    "    evaluator_roc = BinaryClassificationEvaluator(metricName = 'areaUnderROC')\n",
    "    auc_train = evaluator_roc.evaluate(model_preds_train)\n",
    "    auc_valid = evaluator_roc.evaluate(model_preds_valid)\n",
    "    \n",
    "    model_preds_train_df = model_preds_train.select( [ 'userId', 'label', 'probability', 'prediction' ] ).toPandas()\n",
    "    \n",
    "    model_train_fscore = f1_score( model_preds_train_df.label, model_preds_train_df.prediction)\n",
    "    model_train_precision = precision_score( model_preds_train_df.label, model_preds_train_df.prediction)\n",
    "    model_train_recall = recall_score( model_preds_train_df.label, model_preds_train_df.prediction)\n",
    "    model_train_accuracy = accuracy_score( model_preds_train_df.label, model_preds_train_df.prediction)\n",
    "    \n",
    "    model_preds_valid_df = model_preds_valid.select( [ 'userId', 'label', 'probability', 'prediction' ] ).toPandas()  \n",
    "    \n",
    "    model_valid_fscore = f1_score( model_preds_valid_df.label, model_preds_valid_df.prediction)\n",
    "    model_valid_precision = precision_score( model_preds_valid_df.label, model_preds_valid_df.prediction)\n",
    "    model_valid_recall = recall_score( model_preds_valid_df.label, model_preds_valid_df.prediction)\n",
    "    model_valid_accuracy = accuracy_score( model_preds_valid_df.label, model_preds_valid_df.prediction)    \n",
    "    \n",
    "    metrics = {\n",
    "        'train_auc' : auc_train,\n",
    "        'valid_auc' : auc_valid,        \n",
    "        'train_precision' : model_train_precision,\n",
    "        'train_recall' : model_train_recall,\n",
    "        'train_fscore' : model_train_fscore,\n",
    "        'train_accuracy': model_train_accuracy,        \n",
    "        'valid_precision' : model_valid_precision,\n",
    "        'valid_recall' : model_valid_recall,\n",
    "        'valid_fscore' : model_valid_fscore,\n",
    "        'valid_accuracy': model_valid_accuracy\n",
    "    }\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 224 ms, sys: 60.8 ms, total: 285 ms\n",
      "Wall time: 49.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Random forest classifier\n",
    "rf = RandomForestClassifier(\n",
    "        featuresCol = \"ScaledFeatureVector\", \n",
    "        labelCol = 'label', \n",
    "        maxMemoryInMB = 1000, \n",
    "        seed = 42\n",
    "    )\n",
    "rf_model = rf.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- ScaledFeatureVector: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_model.transform(train).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_auc': 0.9857830271216097,\n",
       " 'valid_auc': 0.6671195652173915,\n",
       " 'train_precision': 1.0,\n",
       " 'train_recall': 0.41666666666666669,\n",
       " 'train_fscore': 0.58823529411764708,\n",
       " 'train_accuracy': 0.87116564417177911,\n",
       " 'valid_precision': 1.0,\n",
       " 'valid_recall': 0.125,\n",
       " 'valid_fscore': 0.22222222222222221,\n",
       " 'valid_accuracy': 0.77419354838709675}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model_metrics = evaluate_model(rf_model, train, validation)\n",
    "rf_model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.77 s, sys: 1.01 s, total: 4.78 s\n",
      "Wall time: 5min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gradient boosted trees classifier\n",
    "gbt = GBTClassifier(\n",
    "        featuresCol = \"ScaledFeatureVector\", \n",
    "        labelCol = 'label', \n",
    "        maxMemoryInMB = 1000, \n",
    "        seed = 42,\n",
    "    )\n",
    "gbt_model = gbt.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_auc': 0.9997812773403325,\n",
       " 'valid_auc': 0.7472826086956519,\n",
       " 'train_precision': 1.0,\n",
       " 'train_recall': 0.97222222222222221,\n",
       " 'train_fscore': 0.98591549295774639,\n",
       " 'train_accuracy': 0.99386503067484666,\n",
       " 'valid_precision': 0.59999999999999998,\n",
       " 'valid_recall': 0.375,\n",
       " 'valid_fscore': 0.46153846153846151,\n",
       " 'valid_accuracy': 0.77419354838709675}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_model_metrics = evaluate_model(gbt_model, train, validation)\n",
    "gbt_model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 715 ms, sys: 227 ms, total: 942 ms\n",
      "Wall time: 49.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr = LogisticRegression(featuresCol = \"ScaledFeatureVector\")\n",
    "lr_model = lr.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_auc': 0.8018372703412077,\n",
       " 'valid_auc': 0.6480978260869564,\n",
       " 'train_precision': 0.625,\n",
       " 'train_recall': 0.27777777777777779,\n",
       " 'train_fscore': 0.38461538461538458,\n",
       " 'train_accuracy': 0.80368098159509205,\n",
       " 'valid_precision': 0.5,\n",
       " 'valid_recall': 0.125,\n",
       " 'valid_fscore': 0.20000000000000001,\n",
       " 'valid_accuracy': 0.74193548387096775}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model_metrics = evaluate_model(lr_model, train, validation)\n",
    "lr_model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.23 s, sys: 2.24 s, total: 10.5 s\n",
      "Wall time: 11min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Random forest with cross validation\n",
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [5,10,20]) \\\n",
    "    .addGrid( rf.minInstancesPerNode, [5, 10, 20] ) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=rf_paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(metricName = 'areaUnderROC'),\n",
    "                          numFolds=3)\n",
    "rf_cv_model = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_auc': 0.6968503937007874,\n",
       " 'valid_auc': 0.5353260869565217,\n",
       " 'train_precision': 0.0,\n",
       " 'train_recall': 0.0,\n",
       " 'train_fscore': 0.0,\n",
       " 'train_accuracy': 0.77914110429447858,\n",
       " 'valid_precision': 0.0,\n",
       " 'valid_recall': 0.0,\n",
       " 'valid_fscore': 0.0,\n",
       " 'valid_accuracy': 0.74193548387096775}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_cv_model_metrics = evaluate_model(rf_cv_model, train, validation)\n",
    "rf_cv_model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='RandomForestClassifier_8afa81be378d', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees.'): False,\n",
       " Param(parent='RandomForestClassifier_8afa81be378d', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext'): 10,\n",
       " Param(parent='RandomForestClassifier_8afa81be378d', name='featureSubsetStrategy', doc='The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n].'): 'auto',\n",
       " Param(parent='RandomForestClassifier_8afa81be378d', name='featuresCol', doc='features column name'): 'ScaledFeatureVector',\n",
       " Param(parent='RandomForestClassifier_8afa81be378d', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini',\n",
       " Param(parent='RandomForestClassifier_8afa81be378d', name='labelCol', doc='label column name'): 'label',\n",
       " Param(parent='RandomForestClassifier_8afa81be378d', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 32,\n",
       " Param(parent='RandomForestClassifier_8afa81be378d', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5,\n",
       " Param(parent='RandomForestClassifier_8afa81be378d', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation.'): 1000,\n",
       " Param(parent='RandomForestClassifier_8afa81be378d', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0,\n",
       " Param(parent='RandomForestClassifier_8afa81be378d', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 20,\n",
       " Param(parent='RandomForestClassifier_8afa81be378d', name='numTrees', doc='Number of trees to train (>= 1)'): 5,\n",
       " Param(parent='RandomForestClassifier_8afa81be378d', name='predictionCol', doc='prediction column name'): 'prediction',\n",
       " Param(parent='RandomForestClassifier_8afa81be378d', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities'): 'probability',\n",
       " Param(parent='RandomForestClassifier_8afa81be378d', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name'): 'rawPrediction',\n",
       " Param(parent='RandomForestClassifier_8afa81be378d', name='seed', doc='random seed'): 42,\n",
       " Param(parent='RandomForestClassifier_8afa81be378d', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_cv_model.bestModel.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parallelFitTasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnFolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gradient boosted trees with cross validation\n",
    "gbt_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [5,10]) \\\n",
    "    .addGrid(gbt.minInstancesPerNode, [5, 15] ) \\\n",
    "    .build()\n",
    "\n",
    "gbt_crossval = CrossValidator(estimator=gbt,\n",
    "                          estimatorParamMaps=gbt_paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(metricName = 'areaUnderROC'),\n",
    "                          numFolds=3)\n",
    "\n",
    "gbt_cv_model = gbt_crossval.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gbt_cv_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-fe7433ef7459>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgbt_cv_model_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgbt_cv_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'gbt_cv_model' is not defined"
     ]
    }
   ],
   "source": [
    "gbt_cv_model_metrics = evaluate_model(gbt_cv_model, train, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_cv_model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see what the best models parameters\n",
    "gbt_cv_model.bestModel.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "# Logistic regression with cross validation\n",
    "lr_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.0, 0.1, 0.5, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.05, 0.1] ) \\\n",
    "    .build()\n",
    "\n",
    "lr_crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=lr_paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(metricName = 'areaUnderROC'),\n",
    "                          numFolds=3)\n",
    "\n",
    "lr_cv_model = lr_crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cv_model_metrics = evaluate_model(lr_cv_model, train, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cv_model_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing different models\n",
    "\n",
    "all_metrics = pd.DataFrame( { \n",
    "    'Base RF': rf_model_metrics, \n",
    "    'CV RF' : rf_cv_model_metrics, \n",
    "    'Base GBT' : gbt_model_metrics, \n",
    "    'CV GBT' : gbt_cv_model_metrics,\n",
    "    'Base LR' : lr_model_metrics,\n",
    "    'CV LR' : lr_cv_model_metrics\n",
    "}).sort_index()\n",
    "all_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
